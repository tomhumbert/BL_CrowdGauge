{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Levelness Annotation Creation for a Crowd Sourced Gold Standard\n",
    "\n",
    "BL annotation could range from 0-1\n",
    "0, not basic level\n",
    "1, surely basic level\n",
    "anything inbetween gives estimate to bl likelihood\n",
    "\n",
    "Steps:\n",
    "* prepare rating df blueprint > | synset | blness | accuracy |\n",
    "* DF per 'annotator', create hypothetical binary annotation (bl, or not) \n",
    " - select fastest times in both true/false type triples\n",
    " - is same category + accurate: annotate with 1\n",
    " - is different categories for fastest times + accurate: both get 0.5\n",
    " - is fastest time inaccurate, select next fastest time, give 0.5\n",
    " - two fastest times inaccurate, select remaining, give 0.3\n",
    " - all inaccurate, select superordinate, give 0.1\n",
    "\n",
    "* Create new GOLD STANDARD data frame (per branch) like hollink (compatibility):\n",
    "  -> | synset | glossary | depth in hierarchy | direct hypernyms | direct hyponyms | accuracy | blness |\n",
    "\n",
    "* Per synset, combine blness measure (mean?), add mean accuracy\n",
    "* I would still be interested in an inter-annotator measure that reveals groups of 'similarly' rating annotators. Could use to split gold standard in two w.r.t. groups of ppl.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
