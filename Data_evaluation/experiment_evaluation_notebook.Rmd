---
title: "B.L. Experiment Result Analysis"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
    number_sections: yes
    highlight: espresso
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
---

```{r imports, message=FALSE, include=FALSE}
################################################################################
####        Library imports and setting general theme of the plots          ####
################################################################################

library(dplyr)
library(readr)
library(ggplot2)
library("gridExtra")
library(tidyverse)
library(ggthemes)
library(hash)

old <- theme_set(theme_stata())
```

```{r load data, echo=TRUE, warning=FALSE}

###########################################################################################
# CHANGE THIS > Path to computed results folder here (generated from jupyter notebook)    #
r_folder <- "../Past_experiments/E2/computed_data/"
###########################################################################################

# Declaring files containing answers and participants.
results_file = paste(r_folder,"data_all_ans.csv", sep="")
participant_file = paste(r_folder,"data_all_participants.csv", sep="")

# Loading data
results <- read_csv(results_file, col_types = cols('i','i','l','c','c','f','c','l','f','i','i'))
participants <- read_csv(participant_file, col_types=cols('f','n', 'i','n','n','l', 'l','f','i'))

# Splitting the data set into results from the training round and from the experiment round.
exp_results <- results %>%
  filter(final == T) %>%
  mutate(c_name = str_replace_all(str_remove(c_name, '_n_0[:digit:]'),pattern = '_',replacement = ' ')) %>%
  mutate(img = as.factor(str_replace_all(str_remove(img, '_n_0[:digit:]'),pattern = '_',replacement = ' '))) %>%
  mutate(branch = as.factor(str_replace_all(str_remove(branch, '_n_0[:digit:]'),pattern = '_',replacement = ' '))) %>%
  mutate(stimulus = as.factor(paste(c_name, img, sep = "|")))

train_results <- results %>%
  filter(final == F) %>%
  mutate(c_name = str_replace_all(str_remove(c_name, '_n_0[:digit:]'),pattern = '_',replacement = ' '))  %>%
  mutate(img = as.factor(str_replace_all(str_remove(img, '_n_0[:digit:]'),pattern = '_',replacement = ' '))) %>%
  mutate(branch = as.factor(str_replace_all(str_remove(branch, '_n_0[:digit:]'),pattern = '_',replacement = ' '))) %>%
  mutate(stimulus = as.factor(paste(c_name, img, sep = "|")))

# Some constants needed in latter calculations.
trial_num <- length(exp_results$index)
p_num <- length(unique(exp_results$p_id))
part_id <- sort(c(1:trial_num)%%p_num)+1
stim_num <- trial_num/p_num

```

```{r}
#!!!! Used during Thesis, not generalized
exp1 <- read_csv("../Past_experiments/E1/computed_data/data_all_ans.csv", col_types = cols('i','i','l','c','c','f','c','l','i','i','i')) 
  
exp2 <- read_csv("../Past_experiments/E2/computed_data/data_all_ans.csv", col_types = cols('i','i','l','c','c','f','c','l','i','i','i')) %>%
  filter(final==F) %>%
  mutate(p_id=p_id+200)

all_exp_results <- rbind(exp1, exp2)

p_ids = append(c(1:24),c(201:212))
eng_nat = c(F,F,F,F,F,F,F,T,F,F,F,F,F,F,T,F,F,F,F,F,F,F,F,F,F,T,T,T,T,T,T,T,T,T,F,T)
p_info <- data.frame("p_id"=p_ids, "is_nat_eng"=eng_nat)

all_exp_results <- merge(all_exp_results, p_info, all.x = T) %>%
  mutate(p_id=as.factor(p_id))
```
```{r}
lang_diff <- all_exp_results %>%
  group_by(p_id, is_nat_eng) %>%
  summarise(mean_acc = mean(accurate), mean_rt = mean(react_time))
  
plot_lang_diff <- ggplot(lang_diff, aes(x=mean_acc, y=mean_rt, color=is_nat_eng)) +
  geom_point(size=9) +
  annotate('text', x=lang_diff$mean_acc,y=lang_diff$mean_rt, label=lang_diff$p_id)+
  labs(color="Native English Speaker")

plot_lang_diff

# In the end I told the second group that they should be as accurate as possible, while group 1 tried to be as fast as possible. On these grounds, we may not be able to compare the two language groups. What we can see is an effect between accuracy and reaction time. WHICH ASPECT MAKES SENSE TO HIGHLIGHT WITH COLOR? EXPERIMENT NUMBER?
```

\newpage

# Overview

## Summaries of the columns

```{r table overview, echo=FALSE}
rt_mean <- mean(exp_results$react_time)
acc_mean <- mean(exp_results$accurate)
summary(exp_results[c("stimulus", "lvl", "branch", "p_id", "accurate", "react_time")])
summary(participants$total_time)
```

## Reaction times and mistakes Overview

An overview of the reaction time means, quantiles and outliers with regards to the category levels and also to the stimulus type. A closer look into reaction times can be found in section 4.

```{r basic rt overview, echo=FALSE, fig.height=5, fig.width=6}
plot_basic_results <- ggplot(exp_results, mapping = aes(x=lvl, y=react_time/1000)) + 
  geom_boxplot() + 
  geom_boxplot(data=exp_results, mapping = aes(x=stim_type, y=react_time/1000)) + 
  labs(title = "Basic reaction time overview.") + 
  xlab("Grouping of interest") +
  ylab("Reaction Time (sec)") + 
  scale_x_discrete(limits = c("hypernym","bl","hyponym", "TRUE", "FALSE")) + 
  scale_y_continuous(n.breaks = 10) +
  scale_fill_stata()

plot_basic_results
```

Overview of the amount of mistakes per category branches.

```{r mistake overview per branch, echo=FALSE, fig.height=5, fig.width=6}
mistakes_by_branch <- exp_results %>%
  group_by(stimulus,branch,accurate) %>%
  tally()

plot_mistakes_by_branch <- ggplot(mistakes_by_branch, aes(x=branch, y=n, fill=as.factor(accurate)))+
  geom_col() +
  labs(title = "Trials per branch", subtitle = "Accurate/Inaccurate answers highlighted by color",
       fill="Accurate") +
  xlab("Category Branches") +
  ylab("Count of trials") +
  scale_y_continuous(n.breaks=10) +
  theme(axis.text.x = element_text(vjust = 1, hjust = 0.5, size = 7))

plot_mistakes_by_branch
```
\newpage

## Reaction times and accuracies over time

Do reaction times improve over trials and phases?

```{r fig.height=4, fig.width=6.5}
p_ans_train <- data.frame()
p_ans_exp <- data.frame()
for (ds in 1:2){
  trials = c(1:trial_num*0)
  cumul_rt = c(1:trial_num*0)
  cumul_rt_mean = c(1:trial_num*0)
  cumul_acc = c(1:trial_num*0)
  cumul_acc_mean = c(1:trial_num*0)
  if( ds == 1 ){
    data = exp_results[c("p_id", "accurate", "react_time")]
  }
  else{
    data = train_results[c("p_id", "accurate", "react_time")]
  }
  for(i in 0:(p_num-1)){
    rt_sum <- 0
    acc_sum<- 0
    for(j in 1:stim_num){
      index = (i*stim_num)+j
      if(j == 1){
        rt_sum = data$react_time[index]
        acc_sum = data$accurate[index]
      }
      else{
        rt_sum = rt_sum + data$react_time[index]
        acc_sum = acc_sum + data$accurate[index]
      }
      cumul_rt[index] = rt_sum
      cumul_acc[index] = acc_sum
      trials[index] = j
      cumul_rt_mean[index] = rt_sum/j
      cumul_acc_mean[index] = acc_sum/j
    }
  }
  if( ds == 1 ){
    p_ans_exp <- data
    p_ans_exp$cumul_rt <- cumul_rt/1000
    p_ans_exp$cumul_rt_mean <- cumul_rt_mean/1000
    p_ans_exp$cumul_acc <- cumul_acc
    p_ans_exp$cumul_acc_mean <- cumul_acc_mean
    p_ans_exp$trials <- trials
  }
  else{
    p_ans_train <- data
    p_ans_train$cumul_rt <- cumul_rt/1000
    p_ans_train$cumul_rt_mean <- cumul_rt_mean/1000
    p_ans_train$cumul_acc <- cumul_acc
    p_ans_train$cumul_acc_mean <- cumul_acc_mean
    p_ans_train$trials <- trials
  }
}

plot_rt_over_time_train <- ggplot(p_ans_train, aes(x=trials, y=cumul_rt_mean))+
  geom_smooth(color='black', size=3)+
  geom_line(mapping=aes(color=p_id))+
  scale_y_continuous(name = "Mean reaction time (sec)", n.breaks = 6) +
  scale_x_continuous(name = "Trials in training round", n.breaks = 10) +
  theme(panel.grid.major.x = element_line(size = 0.001),
        legend.position = 0, 
        axis.text.y = element_text(angle=0))+
  coord_cartesian(ylim=c(0.4,1.5))

plot_rt_over_time_exp <- ggplot(p_ans_exp, aes(x=trials, y=cumul_rt_mean))+
  geom_smooth(color='black', size=3)+
  geom_line(mapping=aes(color=p_id))+
  scale_y_continuous(name = "Mean reaction time (sec)", n.breaks = 6) +
  scale_x_continuous(name = "Trials in experiment round", n.breaks = 10) +
  theme(panel.grid.major.x = element_line(size = 0.001),
        legend.position = 0, 
        axis.text.y = element_text(angle=0))+
  coord_cartesian(ylim=c(0.4,1.5))

grid.arrange(plot_rt_over_time_train, plot_rt_over_time_exp)
```

Do accuracies improve over time?
```{r}
plot_acc_over_time_train <- ggplot(p_ans_train, aes(x=trials, y=cumul_acc_mean))+
  geom_smooth(color='black', size=3)+
  geom_line(mapping=aes(color=p_id))+
  scale_y_continuous(name = "Mean accuracy", n.breaks = 6) +
  scale_x_continuous(name = "Trials in training round", n.breaks = 10) +
  theme(panel.grid.major.x = element_line(size = 0.001),
        legend.position = 0, 
        axis.text.y = element_text(angle=0))+
  coord_cartesian(ylim=c(0.5,1))

plot_acc_over_time_exp <- ggplot(p_ans_exp, aes(x=trials, y=cumul_acc_mean))+
  geom_smooth(color='black', size=3)+
  geom_line(mapping=aes(color=p_id))+
  scale_y_continuous(name = "Mean accuracy", n.breaks = 6) +
  scale_x_continuous(name = "Trials in experiment round", n.breaks = 10) +
  theme(panel.grid.major.x = element_line(size = 0.001),
        legend.position = 0, 
        axis.text.y = element_text(angle=0))+
  coord_cartesian(ylim=c(0.5,1))

grid.arrange(plot_acc_over_time_train, plot_acc_over_time_exp)
```

# Accuracy Analysis w.r.t. Participant

This section is concerned with mistakes in the experiment from the participant point of view. It can be used to find unserious participants.

## Overview of Training round
```{r participant ov, echo=FALSE}
participant_means_train <- train_results %>%
  group_by(p_id) %>%
  summarize(mean_rt = mean(react_time), mean_acc = mean(accurate))

summary(participant_means_train[c('p_id', 'mean_rt', 'mean_acc')])
```


## Overview of Experiment round
```{r participant ov, echo=FALSE}
participant_means_exp <- exp_results %>%
  group_by(p_id) %>%
  summarize(mean_rt = mean(react_time), mean_acc = mean(accurate))

summary(participant_means_exp[c('p_id', 'mean_rt', 'mean_acc')])
```

Comparing the mean accuracies and reaction times of all participants.

```{r fig3, echo=FALSE, fig.height=2.5, fig.width=6.5}
m <- mean(participant_means_exp$mean_acc)
data_part <- merge(participant_means_exp,participants,by.x='p_id',by.y='partic_id',all.x = T)

acc_by_participant <- ggplot(data_part, 
                             aes(x=reorder(p_id, desc(-mean_acc)), y=mean_acc, color = eng_native))+
  geom_point() +
  geom_point(data=participant_means_train, aes(x=p_id, y=mean_acc),color='darkgray')+
  xlab("Participant number") +
  scale_y_continuous("Mean Accuracy", labels = scales::percent, n.breaks = 8)+
  geom_hline(yintercept = m, color="#c0c0c0")+
  theme(panel.grid.major.x = element_line(colour="#f4f0ec"), axis.text.y = element_text(angle=0))+
  labs(color="ENG Native")
  
rt_by_participant <- ggplot(data_part, 
                             aes(x=reorder(p_id, desc(-mean_rt)), y=mean_rt, color = eng_native))+
  geom_point() +
  geom_point(data=participant_means_train, aes(x=p_id, y=mean_rt),color='darkgray')+
  xlab("Participant number") +
  scale_y_continuous("Mean Reaction Time", n.breaks = 8)+
  geom_hline(yintercept = m, color="#c0c0c0")+
  theme(panel.grid.major.x = element_line(colour="#f4f0ec"), axis.text.y = element_text(angle=0))+
  labs(color="ENG Native")

grid.arrange(acc_by_participant, rt_by_participant, ncol=2)

```

## Mean Acc improvement

Paired T-test accuracies in training and after.

```{r}
t.test(participant_means_train$mean_acc, participant_means_exp$mean_acc)
t.test(participant_means_train$mean_rt, participant_means_exp$mean_rt)

summary(participants$total_time)
```

Box plots for participant's reaction times on stimuli.

```{r fig4, echo=FALSE}
plot_p_rt <- ggplot(exp_results, aes(x=react_time/1000, y=lvl))+
  geom_boxplot() +
  theme(axis.text.y = element_text(angle=0),
        axis.title.y = element_blank()) +
  labs(title = "Box plots for reaction times of participants.")+
  xlab("Reaction Time (seconds)")+
  scale_x_continuous(n.breaks = 5)+
  coord_cartesian(xlim = c(0.2,1.5))

plot_p_rt

```

Highlighting how participants individually experienced the experiment over time. Each path represents one participant's mistake count as time moves on. Time that passed equally fast for each participant (e.g. showing the stimulus, moving on to the next) is not considered. Only time that passed while the participant was able to answer is considered. Thus, this plot does not show moments where participants took a break during the experiment's break-time screens.

```{r echo=FALSE, fig.height=4, fig.width=6.5}
participant_paths <- exp_results[c("p_id", "accurate", "react_time")]

cumul_err<- c(1:trial_num*0)
cumul_rt <- c(1:trial_num*0)
for(i in 0:(p_num-1)){
  err_sum = 0
  rt_sum = 0
  for(j in (i*stim_num):((i*stim_num)+(stim_num-1))){
    err_sum <- abs(participant_paths$accurate[j+1]-1)+err_sum
    rt_sum <- participant_paths$react_time[j+1]+rt_sum
    cumul_err[j+1] <- err_sum
    cumul_rt[j+1] <- rt_sum
  }
}

#accumulated error count
participant_paths$cumul_err <- cumul_err#/stim_num

#saved as number in minutes
participant_paths$cumul_rt <- cumul_rt/(1000*60)

#creation of df per participant
p_dfs <- hash()
for(i in 1:p_num){
  p_dfs[[paste("p",i, sep="")]] <- filter(participant_paths, p_id==i)
}

#This could have been combined with the previous loop, but this way some paths could be selectively ignored when creating the plot
participant_mistake_over_time <- ggplot(participant_paths, aes(x=cumul_rt, 
                                                               y=cumul_err))

for(p in keys(p_dfs)){
  pp <- p_dfs[[p]]
  lab_x = pp$cumul_rt[length(pp$cumul_rt)]
  lab_y = pp$cumul_err[length(pp$cumul_err)]
  participant_mistake_over_time <- participant_mistake_over_time + 
    geom_line(pp, mapping=aes(x=cumul_rt, y=cumul_err, color=p_id)) +
    annotate(geom = "text", x = lab_x, y = lab_y, label=p, size=3)
}

participant_mistake_over_time <- participant_mistake_over_time +
  scale_x_continuous(name = "Time (minutes)", n.breaks = 10) +
  theme(panel.grid.major.x = element_line(size = 0.001),
        legend.position = 0, axis.text.y = element_text(angle = 0)) +
  ylab("Accumulated mistakes")

participant_mistake_over_time
```

Mistake accumulation over trials vs over time. 

```{r echo=FALSE, fig.height=3, fig.width=7}
ppaths <- exp_results[c("p_id", "accurate")]
ppaths$mistake <- abs(ppaths$accurate-1)
acceptable_num_mistakes <- 0.15*stim_num

trials <- c(1:trial_num*0)
cumul_err<- c(1:trial_num*0)

for(i in 0:(p_num-1)){
  err_sum <- 0
  for(j in 1:stim_num){
    index = (i*stim_num)+j
    if(j == 1){
      err_sum = ppaths$mistake[index]
    }
    else{
      err_sum = err_sum + ppaths$mistake[index]
    }
    cumul_err[index] = err_sum
    trials[index] = j
  }
}

ppaths$cumul_err <- cumul_err
ppaths$trials <- trials

#ppaths_to90 <- ppaths %>%
  #filter(trials < 101) 
#ppaths_after90to330 <- ppaths %>%
  #filter(trials > 100 & trials < 331) 
#ppaths_after330 <- ppaths %>%
  #filter(trials > 330) 

#This could have been combined with the previous loop, but this way some paths could be selectively ignored when creating the plot
participant_mistake_over_time_alt <- ggplot(ppaths, aes(x=trials, y=cumul_err))+
  geom_smooth(color='black', size=3)+
  geom_line(mapping = aes(color=p_id))+
  ylab("Count of mistakes")+
  scale_x_continuous(name = "Number of trials", n.breaks = 10) +
  theme(panel.grid.major.x = element_line(size = 0.001),
        legend.position = 0, 
        axis.text.y = element_text(angle = 0))
  
# These three lines can be used to highlight sections of the trend line
#  geom_smooth(data=ppaths_to90, mapping=aes(x=trials, y=cumul_err), method = 'lm', color='red')+
#  geom_smooth(data=ppaths_after330, mapping=aes(x=trials, y=cumul_err), method = 'lm', color='red')+
#  geom_smooth(data=ppaths_after90to330, mapping=aes(x=trials, y=cumul_err), method = 'lm', color='green')+
  
participant_mistake_over_time_noY <- participant_mistake_over_time +
  theme(axis.title.y = element_blank(),
        axis.text.y =  element_blank(),
        axis.ticks.y = element_blank())

#grid.arrange(grid.arrange(participant_mistake_over_time_alt, participant_mistake_over_time_noY, widths=c(1,1)),participant_rt_over_time, heights=c(2,1))
grid.arrange(participant_mistake_over_time_alt, participant_mistake_over_time_noY, widths=c(1,1))

```


```{r}
data_part <- merge(exp_results,participants,by.x='p_id',by.y='partic_id',all.x = T)

plot_participants <- ggplot(data_part, aes(x=react_time, y=accuracy))+
  geom_smooth()

plot_participants

```

\newpage
# Accuracy Analysis w.r.t. Stimulus

Mistakes to correct answers of all stimuli. Bars in order of how stimuli have been originally ordered.

```{r echo=FALSE, fig.height=2, fig.width=2}
stimOV_dist <- exp_results %>%
  group_by(stimulus,group_index) %>%
  summarise(accuracy = mean(accurate))

plot_mistake_overview_dist <- ggplot(data=stimOV_dist, aes(x=accuracy)) +
  geom_histogram(bins=9) +
  scale_x_continuous(labels = scales::percent) +
  xlab("Accuracy")+
  ylab("Count")+
  labs(fill="Accuracy")
  
plot_mistake_overview_dist

```


What caused mistakes specifically?

\newpage
## Stimuli

Stimuli that received the most erroneous answers. The cutoff is made where 30% of the participants had a mistake.

```{r echo=FALSE, fig.height=4, fig.width=6.5, message=FALSE}
mistake_by_stim <- exp_results %>%
  group_by(stimulus, lvl) %>%
  summarise(error = (1-mean(accurate))) %>%
  filter(error>0.3)

m_by_stim_plot <- ggplot(mistake_by_stim, aes(x=error, y=reorder(stimulus, desc(-error)), fill=lvl)) +
  geom_col() +
  theme(axis.text.y = element_text(angle=0),
        axis.title.y = element_blank(),
        panel.grid.major.x = element_line(size=0.5),
        panel.grid.minor = element_line(size=0.02, colour = "#c0c0c0")) +
  scale_x_continuous(labels=scales::percent) +
  labs(title="High error Stimuli.", fill="Category \nlevel") +
  xlab("Accuracy")


m_by_stim_plot

```
\newpage

## Image

Images that received the most erroneous answers. The cutoff is made at 10% accuracy.

```{r echo=FALSE, fig.height=3, fig.width=6.5, message=FALSE}
mistake_by_img <- exp_results %>%
  group_by(img) %>%
  summarise(error = (1-mean(accurate)))%>%
  filter(error>0.10)

m_by_img_plot <- ggplot(mistake_by_img, aes(x=error, y=reorder(img, desc(-error)))) +
  geom_col() +
  theme(axis.text.y = element_text(angle=0),
        axis.title.y = element_blank(),
        panel.grid.major.x = element_line(size=0.5),
        panel.grid.minor = element_line(size=0.02, colour = "#c0c0c0")) +
  scale_x_continuous(labels=scales::percent) +
  labs(title="Images receiving many mistakes.") +
  xlab("Accuracy")

m_by_img_plot

```
\newpage

## Category Name

Figure 9 shows the category names that received the most erroneous answers. The cutoff is made at 15%.

```{r echo=FALSE, fig.height=4, fig.width=6.5, message=FALSE}
mistake_by_name <- exp_results %>%
  group_by(c_name, lvl) %>%
  summarise(error = (1-mean(accurate)))%>%
  filter(error>0.15)

m_by_name_plot <- ggplot(mistake_by_name, aes(x=error, y=reorder(c_name, desc(-error))), group=lvl) +
  geom_col(aes(fill=lvl)) +
  theme(axis.text.y = element_text(angle=0),
        axis.title.y = element_blank(),
        panel.grid.major.x = element_line(size=0.5),
        panel.grid.minor = element_line(size=0.02, colour = "#c0c0c0")) +
  scale_x_continuous(labels=scales::percent) +
  labs(title="Category names receiving many mistakes.", fill="Level") +
  xlab("Accuracy")

m_by_name_plot

```

\newpage

## Branch

Columns are colored to highlight the share of each category level within the respective branches.

```{r echo=FALSE, fig.height=2, fig.width=6.5, message=FALSE}
mistake_by_branch <- exp_results %>%
  group_by(branch, lvl) %>%
  summarise(error = (1-mean(accurate)))

levels(mistake_by_branch$lvl) <- c("BL", "SUB", "SUP")

m_by_branch_plot <- ggplot(mistake_by_branch, aes(x=error, y=reorder(branch, desc(-error))), group=lvl) +
  geom_col(aes(fill=lvl)) +
  theme(axis.text.y = element_text(angle=0),
        axis.title.y = element_blank(),
        panel.grid.major.x = element_line(size=0.5),
        panel.grid.minor = element_line(size=0.02, colour = "#c0c0c0"),
        legend.position = 0) +
  scale_x_continuous(labels=scales::percent) +
  labs(fill="Level") +
  xlab("Percentage of Error") +
  facet_wrap(mistake_by_branch$lvl)

m_by_branch_plot

```

\newpage

## Level

Accuracy per category level. The columns are colored to highlight the shares of stimulus types. 

```{r echo=FALSE, message=FALSE}
mistake_by_lvl <- exp_results %>%
  group_by(lvl, stim_type) %>%
  summarise(error = (1-mean(accurate)))

m_by_lvl_plot <- ggplot(mistake_by_lvl, 
                        aes(x=error, y=reorder(lvl, desc(-error))), group=stim_type) +
  geom_col(aes(fill=stim_type)) +
  theme(axis.text.y = element_text(angle=0),
        axis.title.y = element_blank(),
        panel.grid.major.x = element_line(size=0.5),
        panel.grid.minor = element_line(size=0.02, colour = "#c0c0c0")) +
  scale_x_continuous(labels=scales::percent) +
  labs(caption="Fig.11. Error rates of category levels.", fill="Stimulus\nType") +
  xlab("Percentage of mistakes over all participants")  

m_by_lvl_plot

```

\newpage

## Stimulus Type

Pie chart of True/False stimulus type's answer accuracies.

```{r echo=FALSE, message=FALSE}
mistake_by_stype <- exp_results %>%
  mutate(accuracy=as.factor(accurate))%>%
  group_by(stim_type, accurate) %>%
  summarize(perc = round((n()/trial_num),2)*100)%>%
  mutate(case=paste(stim_type,ifelse(accurate==1,"positive","negative")))

m_by_stype_plot <- ggplot(mistake_by_stype, 
                        aes(x="",y=perc, fill=case)) +
  geom_bar(stat="identity", width=1, color="black") +
  coord_polar("y", start=0) +
  labs(caption="Fig.12. Error rates of stimulus types.", fill="Case")+
  geom_text(aes(label = paste0(perc, "%")), position = position_stack(vjust=0.5)) +
  labs(x = NULL, y = NULL, fill = NULL)+
  theme(legend.position = "top")

m_by_stype_plot

```


```{r}
mistake_branch <- exp_results %>%
  group_by(branch) %>%
  summarise(err=(1-mean(accurate)))

print(mistake_branch)

```

```{r fig.height=2, fig.width=7}
grid.arrange(m_by_branch_plot,plot_mistake_overview_dist, widths=c(5,2))
```

\newpage

# Experiment component significance analysis in terms of accuracy

```{r message=FALSE, include=FALSE}
aov_data <- exp_results %>%
  group_by(branch, lvl, c_name, img, stimulus, stim_type) %>%
  summarize(err=(1-mean(accurate)))

```

This section calculates the one-way ANOVA and two-way ANOVA of each factor (stimulus components) to determine significant differences between groups. Then, they are pitted against each other in an AIC table to determine the best-fitting statistical model among the ANOVA models.

## One-Way

```{r}
aov_branch <- aov(err ~ branch, data=aov_data)
aov_lvl <- aov(err ~ lvl, data=aov_data)
aov_c_name <- aov(err ~ c_name, data=aov_data)
aov_img <- aov(err ~ img, data=aov_data)
aov_stim <- aov(err ~ stimulus, data=aov_data)
aov_stype <- aov(err ~ stim_type, data=aov_data)

```

### Branch

```{r echo=FALSE}
summary.aov(aov_branch)
```

### Level

```{r echo=FALSE}
summary.aov(aov_lvl)
```

### Category name

```{r echo=FALSE}
summary.aov(aov_c_name)
```

### Image

```{r echo=FALSE}
summary.aov(aov_img)
```

### Stimulus

```{r echo=FALSE}
summary.aov(aov_stim)
```

### Stimulus type

```{r echo=FALSE}
summary.aov(aov_stype)
```

### AIC
```{r echo=FALSE, message=FALSE}
library(AICcmodavg)

candidates <- list(aov_branch,
                  aov_lvl,
                  aov_c_name,
                  aov_img,
                  aov_stim,
                  aov_stype)
nameset <- c("Branch", "Level", "Cat. Name", "Image", "Stimulus", "Stim. Type")

# AIC = 2K – 2ln(L) (K= n# model parameter, L = likelihood)
aictab(candidates, nameset, sort=TRUE)

```

\newpage

## Two-way
The following subsections compare ANOVA of pairs and triples of components that are likely to show significance in another AIC table.
```{r, message=FALSE}
aov_lvl_type <- aov(err ~ lvl + stim_type, data=aov_data)
aov_lvl_branch <- aov(err ~ lvl + branch, data=aov_data)
aov_type_branch <- aov(err ~ stim_type + branch, data=aov_data)
aov_lvl_type_branch <- aov(err ~ lvl + stim_type + branch, data=aov_data)
```

### Level + Type

```{r echo=FALSE}
summary.aov(aov_lvl_type)

```


### Level + Branch

```{r echo=FALSE, message=FALSE}
summary.aov(aov_lvl_branch)

```

### Type + Branch

```{r echo=FALSE, message=FALSE}
summary.aov(aov_type_branch)

```

### Level + Type + Branch

```{r echo=FALSE, message=FALSE}
summary.aov(aov_lvl_type_branch)

```

### AIC
```{r echo=FALSE, message=FALSE}
candidates2 <- list(aov_lvl_type,
                    aov_lvl_branch,
                    aov_type_branch,
                    aov_lvl_type_branch)
nameset2 <- c("Level & Type", "Level & Branch", "Type & Branch", "Level & Type & Branch")

aictab(candidates2, nameset2, sort=TRUE)

```

A Tukey test can be used to measure the differences between group-member pairings.

```{r}
tukey <- TukeyHSD(aov_lvl_branch)

print(tukey)
plot(tukey, sub="Fig.13. Tukey Confidence intervals for level and branch.")
```
\newpage

# Analysis of reaction times

This section is concerned with the reaction times recorded during the experiment. The per-participant reaction times have already been illustrated earlier.

## Overview

To give an overview, we illustrate the distribution of stimuli over the range of reaction times. The columns for the three distinct category levels sit next to each other to facilitate the identification of possible differences. Fig.15 shows box plots for reaction times within the three category levels, one plot per stimulus type. Fig.15 is also a visual representation of table 1, which is the same table Rosch[2] created.

```{r echo=FALSE, fig.height=5, fig.width=12, message=FALSE}
rtOV <- exp_results %>%
  group_by(stimulus, lvl, stim_type, react_time) %>%
  filter(accurate == 1)

sub_mean <- mean(filter(rtOV, lvl == 'hyponym')$react_time)
bl_mean <- mean(filter(rtOV, lvl == 'bl')$react_time)
sup_mean <- mean(filter(rtOV, lvl == 'hypernym')$react_time)
sub_median <- median(filter(rtOV, lvl == 'hyponym')$react_time)
bl_median <- median(filter(rtOV, lvl == 'bl')$react_time)
sup_median <- median(filter(rtOV, lvl == 'hypernym')$react_time)

rtOV_viz <- ggplot(rtOV, aes(x=react_time, color=lvl))+
  geom_freqpoly(size =1.2) +
  scale_x_log10(n.breaks=10, limits=c(NA,2000)) +
  geom_vline(xintercept=sub_mean, color='forestgreen', linetype=2)+
  geom_vline(xintercept =bl_mean, color='red', linetype=2)+
  geom_vline(xintercept=sup_mean, color='deepskyblue', linetype=2)+
  geom_vline(xintercept=sub_median, color='forestgreen')+
  geom_vline(xintercept =bl_median, color='red')+
  geom_vline(xintercept=sup_median, color='deepskyblue')+
  ylab("Number of occurences") +
  xlab("Reaction Time (msec)")+
  labs(title="Distribution of reaction times by category level.", color="Category\nLevel")+
  scale_color_manual(values=c('deepskyblue', 'forestgreen', 'red'), labels=c('Superordinate', 'Subordinate', 'BL'))

rtOV_viz
```

```{r echo=FALSE, message=FALSE}
rt_box <- ggplot(rtOV, aes(x=react_time, fill=branch))+
  geom_boxplot()+
  facet_grid(rtOV$lvl)+
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major.x = element_line(),
        panel.grid.minor.x = element_line(color="#c0c0c0", size=0.5))+
  scale_x_continuous(n.breaks = 14) +
  labs(fill="", 
       title="Box plots of reaction time means over all stimuli") +
  xlab("Reaction Time (msec)")

rt_box

```

```{r Rosch table, echo=FALSE, message=FALSE, warning=FALSE}
library(reshape2)
  #filter(p_id != 1) %>%
  #filter(index>100) %>%

rtable <- exp_results %>%
  filter(react_time < 10000)%>%
  group_by(lvl, stim_type) %>%
  summarize(rt_mean = mean(react_time), rt_median = median(react_time)) 

rmatrix <- matrix(c(1:6),nrow = 2, ncol=3,dimnames = list(c("T", "F"),c("Superordinate", "Basic Level", "Subordinate")))

for(i in 1:6){
  lev <- rtable$lvl[i]
  typ <- rtable$stim_type[i]
  val <- rtable$rt_mean[i]
  
  if (lev == "hypernym" & typ == TRUE){
    rmatrix['T',"Superordinate"] <- val
  } else if(lev == "hypernym" & typ == FALSE){
    rmatrix['F',"Superordinate"] <- val
  } else if(lev == "bl" & typ == TRUE){
    rmatrix['T',"Basic Level"] <- val
  } else if(lev == "bl" & typ == FALSE){
    rmatrix['F',"Basic Level"] <- val
  } else if(lev == "hyponym" & typ == TRUE){
    rmatrix['T',"Subordinate"] <- val
  } else if(lev == "hyponym" & typ == FALSE){
    rmatrix['F',"Subordinate"] <- val
  }
}

print(rmatrix)
```
Table 1.: Matrix showing the mean reaction times at different category levels and stimuli types.

```{r}
rmatrix <- matrix(c(1:6),nrow = 2, ncol=3,dimnames = list(c("T", "F"),c("Superordinate", "Basic Level", "Subordinate")))

for(i in 1:6){
  lev <- rtable$lvl[i]
  typ <- rtable$stim_type[i]
  val <- rtable$rt_median[i]
  
  if (lev == "hypernym" & typ == TRUE){
    rmatrix['T',"Superordinate"] <- val
  } else if(lev == "hypernym" & typ == FALSE){
    rmatrix['F',"Superordinate"] <- val
  } else if(lev == "bl" & typ == TRUE){
    rmatrix['T',"Basic Level"] <- val
  } else if(lev == "bl" & typ == FALSE){
    rmatrix['F',"Basic Level"] <- val
  } else if(lev == "hyponym" & typ == TRUE){
    rmatrix['T',"Subordinate"] <- val
  } else if(lev == "hyponym" & typ == FALSE){
    rmatrix['F',"Subordinate"] <- val
  }
}

print(rmatrix)
```
## Analysis of Variation

This section applies the two-way ANOVA model on the measurements. The category level (between-subject fixed effect) and the category names (random variable) are used as independent variables.

```{r message=FALSE, include=FALSE}
rt_true <- exp_results %>%
  filter(stim_type == TRUE)%>%
  filter(react_time<9000) %>%
  select(lvl, c_name, react_time) %>%
  mutate(c_name = as.factor(c_name))
  
rt_false <- exp_results %>%
  filter(stim_type == F)%>%
  filter(react_time<9000) %>%
  select(lvl, c_name, react_time) %>%
  mutate(c_name = as.factor(c_name))
  

roschAOV_true <- aov(react_time ~ lvl + c_name, data = rt_true)
roschAOV_false <- aov(react_time ~ lvl + c_name, data = rt_false)

tukey_r_true <- TukeyHSD(roschAOV_true, which = 'lvl')
tukey_r_false <- TukeyHSD(roschAOV_false, which = 'lvl')

```

### True type stimuli

Anova summary:
```{r}
summary.aov(roschAOV_true)
```

Tukey test:
```{r}
tukey_r_true

plot(tukey_r_true)
```

### False type stimuli

Anova summary:
```{r}
summary.aov(roschAOV_false)
```

Tukey test:
```{r}
tukey_r_false

plot(tukey_r_false)
```

\newpage

# B.L. Determination

Ultimately, the experiment is conducted to determine the basic level category name from a selection of three candidates. These triples form a path through one of the WordNet branches. We only consider a single element in this path basic level. Basic levelness can only be infered from reaction times of the true type stimuli. The false type stimuli reactions say more about the image than about the category names.

## Simple rating

In this section, we see a simple implementation to declare which category name from the triple is the basic level (super- and subordinates can be inferred). For each triple, the category name with the fastest mean reaction time is chosen as basic level. Below, you can see the top nine results of the evaluation. Thereafter follows a confusion matrix of expected and predicted levels.

```{r echo=FALSE, message=FALSE, warning=TRUE}
df_for_rating <- exp_results %>%
  filter(stim_type == T) %>%
  group_by(lvl, c_name, branch, img) %>%
  summarise(rt_mean = mean(react_time), acc_mean = mean(accurate)) %>%
  mutate(lvl = ifelse(lvl=='hyponym', 'sub', ifelse(lvl=='hypernym', 'super', 'bl')))

all_img <- unique(df_for_rating$img)
df_rated_simple <- data.frame()

for (imag in all_img){
  triple <- df_for_rating %>%
    filter(img == imag)
  blvl <- triple[which(triple$rt_mean == min(triple$rt_mean)),]
  triple <- triple[which(triple$rt_mean != min(triple$rt_mean)),]
  blvl$proj_lvl <- 'bl'
  
  if(blvl$lvl == 'super'){
    triple$proj_lvl <- 'sub'
    df_rated_simple <- rbind(df_rated_simple, blvl)
    df_rated_simple <- rbind(df_rated_simple, triple)
    
  } else if(blvl$lvl == 'bl'){
    sublvl <- triple[which(triple$lvl == "sub"),]
    sublvl$proj_lvl <- "sub"
    suplvl <- triple[which(triple$lvl == "super"),]
    suplvl$proj_lvl <- "super"
    df_rated_simple <- rbind(df_rated_simple, suplvl)
    df_rated_simple <- rbind(df_rated_simple, blvl)
    df_rated_simple <- rbind(df_rated_simple, sublvl)
  } else{
    triple$proj_lvl <- 'super'
    df_rated_simple <- rbind(df_rated_simple, blvl)
    df_rated_simple <- rbind(df_rated_simple, triple)
  }
  
}

df_rated_simple$lvl <- as.factor(df_rated_simple$lvl)
df_rated_simple$proj_lvl <- as.factor(df_rated_simple$proj_lvl)

head(df_rated_simple[c("c_name", "lvl", "proj_lvl")], n = 9)

```

```{r echo=FALSE, message=FALSE}
library(caret)

con_mat_bl <- confusionMatrix(data=df_rated_simple$proj_lvl, reference = df_rated_simple$lvl)
con_mat_bl
```

## Simple rating, but majority agreement is used instead of rt mean

```{r}
df_for_rating <- exp_results %>%
  group_by(lvl, c_name, branch, img, stim_type, p_id) %>%
  mutate(lvl = ifelse(lvl=='hyponym', 'sub', ifelse(lvl=='hypernym', 'super', 'bl')))

all_img <- unique(df_for_rating$img)
df_rated_simpleplus <- data.frame()

for (imag in all_img){
  triple <- df_for_rating %>%
    filter(img == imag)
  
  is_bl_to_participant = data.frame()
  
  for(i in 1:p_num){
    bl <- triple[which(triple$react_time == min(triple$react_time)),]
    is_bl_to_participant <- rbind(is_bl_to_participant, bl)
  }
  
  bl_tally <- data.frame(is_bl_to_participant) %>%
    group_by(c_name)%>%
    tally()
  
  blvl <- triple[which(triple$rt_mean == min(triple$rt_mean)),]
  triple <- triple[which(triple$rt_mean != min(triple$rt_mean)),]
  blvl$proj_lvl <- 'bl'
  
  if(blvl$lvl == 'super'){
    triple$proj_lvl <- 'sub'
    df_rated_simpleplus <- rbind(df_rated_simpleplus, blvl)
    df_rated_simpleplus <- rbind(df_rated_simpleplus, triple)
    
  } else if(blvl$lvl == 'bl'){
    sublvl <- triple[which(triple$lvl == "sub"),]
    sublvl$proj_lvl <- "sub"
    suplvl <- triple[which(triple$lvl == "super"),]
    suplvl$proj_lvl <- "super"
    df_rated_simpleplus <- rbind(df_rated_simpleplus, suplvl)
    df_rated_simpleplus <- rbind(df_rated_simpleplus, blvl)
    df_rated_simpleplus <- rbind(df_rated_simpleplus, sublvl)
  } else{
    triple$proj_lvl <- 'super'
    df_rated_simpleplus <- rbind(df_rated_simpleplus, blvl)
    df_rated_simpleplus <- rbind(df_rated_simpleplus, triple)
  }
  
}

df_rated_simpleplus$lvl <- as.factor(df_rated_simpleplus$lvl)
df_rated_simpleplus$proj_lvl <- as.factor(df_rated_simpleplus$proj_lvl)

head(df_rated_simpleplus[c("c_name", "lvl", "proj_lvl")], n = 9)

```

## Advanced rating

### Import rated data set from jupyter notebook

```{r}
adv_rated_data = read_csv("../Past_experiments/Pilot2/computed_data/Gold_Standard_with_rt_and_blness.csv")

adv_rated_data <- adv_rated_data %>%
  mutate(old_bl_label = old_bl_label == 1) %>%
  mutate(blness_binary = floor(blness)==1)


summary(adv_rated_data[c('branch', 'synset', 'expert_certainty', 'old_bl_label', 'blness_binary', 'sample_median_rt', 'sample_mean_acc', 'sample_std_deviation')])
```

```{r}
con_mat_bl <- confusionMatrix(data=as.factor(adv_rated_data$blness_binary), reference = as.factor(adv_rated_data$old_bl_label), dnn = c("Crowd Labels", "Expert Labels"))
con_mat_bl

```

Comparison in terms of levels
* Infer level label again (old and new)
```{r}


```


## RT and Accuracy

```{r}
rt_acc <- exp_results %>%
  filter(p_id != '1') %>%
  mutate(type = ifelse(stim_type * accurate == 1, 1,ifelse(stim_type & accurate == 0, 3,ifelse(!stim_type & accurate == 1, 2, 4)))) %>%
  group_by(react_time, p_id, c_name, type) %>%
  count() %>%
  mutate(type = as.factor(type)) %>%
  filter(c_name != 'furniture_n_01' & c_name != 'clothing_n_01' & c_name != 'edible_fruit_n_01' & c_name != 'musical_instrument_n_01' & c_name != 'hand_tool_n_01')%>%
  filter(react_time < 8000)

rated_df <- df_rated_simple[c('c_name', 'lvl')] %>%
  filter(c_name != 'furniture_n_01' & c_name != 'clothing_n_01' & c_name != 'edible_fruit_n_01' & c_name != 'musical_instrument_n_01' & c_name != 'hand_tool_n_01')

rated_df_rts <- merge(rt_acc, rated_df, all.x=T)

effect_comparison <- ggplot(data=rated_df_rts, 
                            mapping=aes(x=react_time, y=c_name), color=lvl) +
  geom_point() +
  facet_wrap(rated_df_rts$type)

effect_comparison
```


\newpage

# References

[1] Gagné, N., & Franzen, L., 2021, https://doi.org/10.31234/osf.io/nt67j

[2] E. Rosch, C. B. Mervis, W. D. Gray, D. M. Johnson, and P. Boyes-Braem,
“Basic objects in natural categories,” Cognitive Psychology, vol. 8, no. 3,
pp. 382–439, 1976.